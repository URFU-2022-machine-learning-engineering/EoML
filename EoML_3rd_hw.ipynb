{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Итоговый проект по предмету \"Этика искусственного интеллекта\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ubgqdIt3iUin",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:36.353437836Z",
     "start_time": "2023-12-31T17:49:35.961926285Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Импорт данных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:36.669138799Z",
     "start_time": "2023-12-31T17:49:36.393554933Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       id    target                                       comment_text  \\\n0   59856  0.893617               haha you guys are a bunch of losers.   \n1  239607  0.912500  Yet call out all Muslims for the acts of a few...   \n2  239612  0.830769  This bitch is nuts. Who would read a book by a...   \n3  240311  0.968750                                   You're an idiot.   \n4  240329  0.900000  Who cares!? Stark trek and Star Wars fans are ...   \n\n   severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n0         0.021277  0.000000         0.021277  0.872340  0.0000    0.0   \n1         0.050000  0.237500         0.612500  0.887500  0.1125    0.0   \n2         0.107692  0.661538         0.338462  0.830769  0.0000    0.0   \n3         0.031250  0.062500         0.000000  0.968750  0.0000    NaN   \n4         0.100000  0.200000         0.000000  0.900000  0.0000    NaN   \n\n   atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n0      0.0  ...        2006  rejected      0    0    0      1         0   \n1      0.0  ...       26670  approved      0    0    0      1         0   \n2      0.0  ...       26674  rejected      0    0    0      0         0   \n3      NaN  ...       32846  rejected      0    0    0      0         0   \n4      NaN  ...       32846  rejected      0    0    0      0         0   \n\n   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n0         0.000000                         4                        47  \n1         0.000000                         4                        80  \n2         0.061538                         4                        65  \n3         0.000000                         0                        32  \n4         0.300000                         0                        10  \n\n[5 rows x 45 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>...</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59856</td>\n      <td>0.893617</td>\n      <td>haha you guys are a bunch of losers.</td>\n      <td>0.021277</td>\n      <td>0.000000</td>\n      <td>0.021277</td>\n      <td>0.872340</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>239607</td>\n      <td>0.912500</td>\n      <td>Yet call out all Muslims for the acts of a few...</td>\n      <td>0.050000</td>\n      <td>0.237500</td>\n      <td>0.612500</td>\n      <td>0.887500</td>\n      <td>0.1125</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>26670</td>\n      <td>approved</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>4</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>239612</td>\n      <td>0.830769</td>\n      <td>This bitch is nuts. Who would read a book by a...</td>\n      <td>0.107692</td>\n      <td>0.661538</td>\n      <td>0.338462</td>\n      <td>0.830769</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>26674</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.061538</td>\n      <td>4</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>240311</td>\n      <td>0.968750</td>\n      <td>You're an idiot.</td>\n      <td>0.031250</td>\n      <td>0.062500</td>\n      <td>0.000000</td>\n      <td>0.968750</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>32846</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>240329</td>\n      <td>0.900000</td>\n      <td>Who cares!? Stark trek and Star Wars fans are ...</td>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.000000</td>\n      <td>0.900000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>32846</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.300000</td>\n      <td>0</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 45 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:36.679739863Z",
     "start_time": "2023-12-31T17:49:36.671816223Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "comments = data[\"comment_text\"]\n",
    "target = (data[\"target\"] > 0.7).astype(int)"
   ],
   "metadata": {
    "id": "zQQQ8S4Fpzuf",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:36.731675587Z",
     "start_time": "2023-12-31T17:49:36.680244340Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 1\n",
    "### Разделение данных на train и test"
   ],
   "metadata": {
    "id": "-jMTeEnErPoP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(comments, target, train_size=0.7)"
   ],
   "metadata": {
    "id": "_NVPem5prhM1",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:36.759184211Z",
     "start_time": "2023-12-31T17:49:36.704112561Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErlpA13vtowi",
    "outputId": "05136005-11ea-4bb0-c9b6-d77b60635624",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:36.788742332Z",
     "start_time": "2023-12-31T17:49:36.746539470Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "((63631,), (27271,), (63631,), (27271,))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 2"
   ],
   "metadata": {
    "id": "dvjyimRosEoM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Векторизация данных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ],
   "metadata": {
    "id": "rmOpxMhytWUC",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:38.206249477Z",
     "start_time": "2023-12-31T17:49:36.788570103Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train_vec"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tF7l1sG8thBw",
    "outputId": "c4ab0fcc-32b0-48dd-9432-38e732ddbcd0",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:38.210427949Z",
     "start_time": "2023-12-31T17:49:38.207803384Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<63631x57895 sparse matrix of type '<class 'numpy.int64'>'\n\twith 2148649 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 3"
   ],
   "metadata": {
    "id": "KQW7bitvt8h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Определение модели и её обучение"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lr = LogisticRegression(random_state=42, max_iter=2000)\n",
    "lr.fit(X_train_vec, y_train)\n",
    "predict = lr.predict(X_test_vec)"
   ],
   "metadata": {
    "id": "PS0SL-AHuCfi",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:46.013166048Z",
     "start_time": "2023-12-31T17:49:38.210032044Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'Accuracy: {accuracy_score(predict, y_test) * 100:.2f}%')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLr4qebwwQfF",
    "outputId": "73fa039e-e87e-4112-88cb-1d6c9c3ae2c2",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:46.063177045Z",
     "start_time": "2023-12-31T17:49:46.018566029Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.80%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 4"
   ],
   "metadata": {
    "id": "kWQVnTTky8VI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Определение функции предсказания токсичности комментария"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_toxicity(comment, vectorizer, model):\n",
    "    \"\"\"\n",
    "    This function predicts the toxicity of a given comment.\n",
    "\n",
    "    Parameters:\n",
    "    comment (str): The comment to be evaluated.\n",
    "    vectorizer (CountVectorizer): The vectorizer object used for transforming the comment.\n",
    "    model (LogisticRegression): The trained model used for making the prediction.\n",
    "\n",
    "    Returns:\n",
    "    float: The probability of the comment being toxic.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Return the probability\n",
    "    return model.predict_proba(vectorizer.transform([comment]))[:, 1][0]"
   ],
   "metadata": {
    "id": "s1UAAQftzWOI",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:46.065240759Z",
     "start_time": "2023-12-31T17:49:46.035612548Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "comment = \"Name is an idiot\"\n",
    "toxicity_probability = predict_toxicity(comment, vectorizer, lr)\n",
    "print(f\"Comment toxicity: {toxicity_probability:.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Grn9uKpiys7C",
    "outputId": "0b71b8a2-8bc0-4654-cbdf-32f035e35dda",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:46.085346912Z",
     "start_time": "2023-12-31T17:49:46.055129715Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment toxicity: 1.00\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "comment = \"Make America great again\"\n",
    "toxicity_probability = predict_toxicity(comment, vectorizer, lr)\n",
    "print(f\"Comment toxicity: {toxicity_probability:.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAdRS5xM1ViA",
    "outputId": "42769756-1e04-4fbf-8ba4-3b09efaeb828",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:49:46.125692143Z",
     "start_time": "2023-12-31T17:49:46.084117658Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment toxicity: 0.08\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 5"
   ],
   "metadata": {
    "id": "_ZGn8Ya53R5Y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Проверка работы модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text = input(\"Input some text and we'll try to predict it toxicity: \")\n",
    "\n",
    "print(f\"For comment '{text}' toxicity was: {predict_toxicity(text, vectorizer, lr):.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lcbM70q2_xL",
    "outputId": "b926d968-84f1-4657-f75e-0048dc0a40f7",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:50:15.522611966Z",
     "start_time": "2023-12-31T17:49:46.125547274Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For comment 'SkillFactory' toxicity was: 0.17\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 6"
   ],
   "metadata": {
    "id": "Mzk7y1523qgB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Вывести 10 наиболее токсичных комментариев"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a dictionary of words and their coefficients, and sort it\n",
    "toxic_words = sorted({word: cf for word, cf in zip(vectorizer.vocabulary_.keys(), lr.coef_[0])}.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "# Print the words and their coefficients\n",
    "for word, coefficient in toxic_words:\n",
    "    print(f'Word: {word}, Coefficient: {coefficient}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdP13jUhIJjt",
    "outputId": "c2953bad-c27f-4cee-80da-cf32202a96d2",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:50:15.523022098Z",
     "start_time": "2023-12-31T17:50:15.522575669Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: décor, Coefficient: 9.281213324494548\n",
      "Word: eia, Coefficient: 8.770564908432828\n",
      "Word: handguns, Coefficient: 8.699503300255174\n",
      "Word: ursula, Coefficient: 7.536900912018221\n",
      "Word: terns, Coefficient: 6.886680247086995\n",
      "Word: outcomes, Coefficient: 6.726174951446813\n",
      "Word: ennui, Coefficient: 6.526442670611855\n",
      "Word: percipitation, Coefficient: 6.481518811650342\n",
      "Word: trumpoids, Coefficient: 6.418134372716222\n",
      "Word: cried, Coefficient: 6.280913215783329\n",
      "Word: wastes, Coefficient: 6.177540970690808\n",
      "Word: gollum, Coefficient: 6.16750906139861\n",
      "Word: herschel, Coefficient: 6.008186321324067\n",
      "Word: sharpened, Coefficient: 5.985565198485761\n",
      "Word: overides, Coefficient: 5.8430353522856855\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 7\n",
    "\n",
    "При перезапуске блокнота коэффициенты могут измениться, привожу пример первого вывода:\n",
    "Изначально, мне все комментарии показались неуместными, пришлось провести небольшой research.\n",
    "\n",
    "- Hom: Это может быть сокращением или опечаткой оскорбительного термина, что может объяснить его высокий коэффициент.\n",
    "- Impetus (импетус): Нейтральное слово, обычно используемое в формальных или академических контекстах. Его высокий уровень токсичности удивителен и может указывать на предвзятость или недостаток модели.\n",
    "- Hanger (вешалка): Общее, не оскорбительное слово. Его связь с токсичностью может быть связана с конкретными контекстами, которые здесь не очевидны.\n",
    "- Yamulkes (ярмулки): Слово, относящееся к религиозному предмету. Его высокий уровень токсичности может свидетельствовать о неправильном использовании в оскорбительных или дискриминационных контекстах, или может быть артефактом модели.\n",
    "- Californian (калифорниец): Демоним; его присутствие здесь предполагает возможное использование в негативных или стереотипных контекстах.\n",
    "- Clang (звон): В целом нейтральное слово, связанное со звуком. Его высокий балл может быть контекстуальным или аномальным.\n",
    "- UAF: Акроним, который может иметь несколько значений. Без контекста трудно оценить, почему он считается токсичным.\n",
    "- Pew (лавка): Нейтральное слово, часто связанное с сиденьями в церквях. Его токсичность может быть специфичной для контекста.\n",
    "- Warden (начальник тюрьмы): Обычно связано с тюрьмами, его присутствие может быть связано с негативными контекстами в обсуждениях.\n",
    "- Khalid: Личное имя, что предполагает, что его присутствие может быть связано с целенаправленными домогательствами или неправильным использованием.\n",
    "- Headwaters (истоки): Нейтральный термин в географии или экологической науке. Его высокий балл неожидан и может указывать на аномалию модели.\n",
    "- Hatriotism (ненавистный патриотизм): По-видимому, это словообразование от \"ненависть\" и \"патриотизм\". Этот неологизм, вероятно, имеет негативные коннотации, что объясняет его показатель токсичности.\n",
    "- RAF: Акроним с различными значениями, включая \"Королевские ВВС\". Токсичность может зависеть от контекста.\n",
    "- Dug (выкопанный): Прошедшее время от \"копать\". Нейтральное слово, его присутствие здесь необычно и может указывать на контекстуальное использование.\n",
    "- Included (включенный): Общее, нейтральное слово. Его высокий уровень токсичности довольно удивителен и может указывать на ошибку моделирования.\n",
    "\n",
    "В заключение, некоторые слова (например, \"hatriotism\" или потенциально оскорбительные термины) могут по своей сути появляться в токсичных контекстах, но многие другие (например, \"impetus\", \"hanger\", \"pew\") вероятно отражают контекстуальное использование или потенциальные аномалии в модели. Крайне важно пересмотреть и усовершенствовать модель, учитывая контекст использования и потенциальные предвзятости."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 8"
   ],
   "metadata": {
    "id": "bB6y-tGrPM1M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Проверить как алгоритм работает на некоторых комментариях."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Давайте посмотрим, как ваш алгоритм классифицирует следующие комментарии:\n",
    "\n",
    "\"I have a christian friend\"\n",
    "\"I have a muslim friend\"\n",
    "\"I have a white friend\"\n",
    "\"I have a black friend\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "comments_list = [\"I have a christian friend\", \"I have a muslim friend\", \"I have a white friend\", \"I have a black friend\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:50:15.547797705Z",
     "start_time": "2023-12-31T17:50:15.522661569Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "for comment in comments_list:\n",
    "    print(f\"Токсичность комментария {comment}: {predict_toxicity(comment, vectorizer, lr):.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXIv5FhEJr-z",
    "outputId": "fd61ab39-2d32-4e6c-c136-60c86af45b7e",
    "ExecuteTime": {
     "end_time": "2023-12-31T17:50:15.574482840Z",
     "start_time": "2023-12-31T17:50:15.540368414Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токсичность комментария I have a christian friend: 0.14\n",
      "Токсичность комментария I have a muslim friend: 0.45\n",
      "Токсичность комментария I have a white friend: 0.34\n",
      "Токсичность комментария I have a black friend: 0.54\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Выводы:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Результаты модели предсказания токсичности комментариев интересные и могут указывать на потенциальный смещенный (biased) характер модели. Рассмотрим каждый из результатов:\n",
    "\n",
    "1) \"I have a christian friend\" (0.21): Это заявление имеет самый низкий показатель токсичности, что может отражать меньшую частоту негативных или стереотипных утверждений о христианах в данных, на которых обучалась модель.\n",
    "2) \"I have a muslim friend\" (0.53): Более высокий уровень токсичности может быть связан с более высокой частотой стереотипных или негативных высказываний о мусульманах в обучающих данных.\n",
    "3) \"I have a white friend\" (0.42) и \"I have a black friend\" (0.59): Заметно, что утверждение о черном друге имеет выше показатель токсичности, что может указывать на наличие предвзятости в данных, связанной с расовыми вопросами.\n",
    "\n",
    "Эти результаты могут отражать смещенность (bias) в обучающих данных модели. Модели машинного обучения часто усваивают и воспроизводят существующие предрассудки и стереотипы из данных, на которых они обучаются. Если в обучающих данных преобладают негативные комментарии, связанные с определенными группами людей (например, определенные религии или расы), модель может неправильно интерпретировать нейтральные утверждения как токсичные.\n",
    "\n",
    "По поводу этичности: использование такой модели может быть неэтичным, если она применяется в реальных условиях без должного понимания и коррекции ее предвзятости. Важно провести тщательный анализ данных и возможно переобучить модель на более сбалансированном наборе данных, чтобы уменьшить смещенность и улучшить точность предсказаний. Это включает в себя анализ не только самих текстов, но и контекста, в котором они используются."
   ],
   "metadata": {
    "id": "4DfbJxuvoymT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 9\n",
    "\n",
    "Я считаю, что я дал ответ на это задание выше.\n",
    "Хотел бы только сделать ремарку про высказывание с платформы:\n",
    "\n",
    "\"Вы заметили, что комментарии, относящиеся к исламу, с большей вероятностью будут токсичными, чем комментарии, относящиеся к другим религиям, поскольку онлайн-сообщество исламофобно.\"\n",
    "\n",
    "Это высказывание по моему не этично. Я тоже часть интернет сообщества и однако я не исламофоб, те же кто это писали, приравняли всё интернет сообщество к исламофобам, что является очень поверхностным суждением. Я считаю, что это высказывание не этично и не корректно."
   ],
   "metadata": {
    "id": "HUI0tXqTVSiT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Я также считаю что я ответил на вопрос в прошлом задании, но дам более равномерный ответ тут.\n",
    "\n",
    "Для улучшения алгоритма предсказания токсичности комментариев в направлении его этического применения, можно рассмотреть следующие идеи:\n",
    "\n",
    "1. Использование более сбалансированных и разнообразных обучающих данных: Одна из основных проблем в работе с алгоритмами машинного обучения — это предвзятость в обучающих данных. Если данные содержат стереотипы или предубеждения, модель может воспроизводить эти же предубеждения. Решением может быть сбор данных из разнообразных источников и культур, что помогает уменьшить одностороннюю предвзятость. Также полезно включать данные, которые явно контрастируют с распространенными стереотипами, для обучения модели распознавать и отвергать предубеждения.\n",
    "\n",
    "2. Контекстуальный анализ: Многие слова и фразы могут быть токсичными или не токсичными в зависимости от контекста. Развитие и применение алгоритмов, способных анализировать контекст, в котором используется язык, может значительно повысить точность и этичность модели. Это включает в себя улучшение понимания моделью намерений пользователя, семантических и культурных нюансов, а также различных форм использования языка (ирония, сарказм, шутки).\n",
    "\n",
    "Применение этих подходов может помочь создать более справедливую, объективную и этичную систему, которая лучше понимает и интерпретирует человеческий язык в его многообразии."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В качестве заключения могу добавить, что модель вроде бы показавшую 0.9 accuracy абсолютно не точна на практике, и не этична, что поднимает проблему тестирования моделей и возможно выбора метрики. Поскольку лучшим тестом на данный момент является человек, это тоже поднимает вопрос human-in-the-loop, поскольку компании получающие миллиардные инвестиции зачастую платят копейки своим тестировщикам и модераторам. \n",
    "\n",
    "Это предложение добавил GitHub Copilot, я решил его оставить и можно считать его \"голосом из машины\".\n",
    "\"Возможно стоит ввести какие-то стандарты для тестирования моделей, и ввести какие-то стандарты для тестировщиков моделей, чтобы они не были бесплатными рабами, а получали достойную оплату за свою работу.\""
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
